{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "We are aiming to develop a software library for ***automatic differentiation*** (AD), which is a technique to numerically evaluate the derivative of a given function.\n",
    "\n",
    "Derivative is the key to many scientific and engineering problems, like numerical simulations and optimizations, so that the accuracy and speed of its evaluation are very important. Unlike symbolic or numerical differentiation, AD is much more computationally efficient with less errors and it is much faster at computing partial derivatives of functions with many inputs, which makes AD a superior method for derivative evaluations in large-scale scientific problems to other classic methods.\n",
    "\n",
    "### Background\n",
    "**Decomposition into elementary functions**\n",
    "\n",
    "One of the basic ideas behind AD is that given any complicated functions, we can decompose it into elementary ones. In other words, we can consider any function as a combination of elementary functions with elementary arithmetic operations, like addition and multiplication. The elementary functions include but are not limited to powers ($x^k$), roots ($\\sqrt{x}$), exponential functions ($e^{x}$), logarithms ($\\log{x}$), trigonometric functions ($\\sin{x}$), etc.\n",
    "\n",
    "**Computational graph**\n",
    "\n",
    "The realization of function decomposition is through computational graph. Each node represents an elementary arithmetic operation, or an elementary function, while each directed edge represents a path, or the flow of information. The computational graph can be used for both function value evaluation and derivative evaluation, where function value evalution only requires edge to deliver the value of the source node, while derivative evaluation also stores derivative on edges.\n",
    "\n",
    "**Chain rule**\n",
    "\n",
    "The fundamental to AD is the decomposition of differentials provided by the chain rule, which has the following formula：\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial t} = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial t},$$ where we have a function $f = f(x(t))$. By applying the chain rule repeatedly on the computational graph, derivatives of arbitrary order can be computed automatically and accurately with low computational complexity。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use *AutoDiff_CKMZ*\n",
    "\n",
    "As our package will be distributed through PyPI, users can install it using `pip`:\n",
    "```python\n",
    "pip install AutoDiff_CKMZ\n",
    "```\n",
    "\n",
    "To use forward mode of automatic differentiation, users should import the `AutoDiff` module. To use the reverse mode of automatic differentiation, `back_prop` module should be imported. They can also import all the modules in the package.\n",
    "```python\n",
    "from AutoDiff_CKMZ.modules import AutoDiff\n",
    "from AutoDiff_CKMZ.modules import back_prop\n",
    "```\n",
    "The AD object is instantiated in the following way\n",
    "```python\n",
    "import AutoDiff_CKMZ.modules.AutoDiff as AD\n",
    "obj = AD.AutoDiff(2)\n",
    "```\n",
    "Basic Demo\n",
    "```python\n",
    "f = lambda x: 2*x+1\n",
    "result = f(obj)\n",
    "print(result.x, result.dx)\n",
    "> 5 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Organization\n",
    "\n",
    "The directory structure will look like \n",
    "```\n",
    "AutoDiff_CKMZ/\n",
    "    AutoDiff_CKMZ/\n",
    "        modules/  \n",
    "            __init__.py\n",
    "            AutoDiff.py\n",
    "            back_prop.py\n",
    "            tests/\n",
    "                __init__.py\n",
    "                test_fwd.py\n",
    "                test_bp.py\n",
    "    README.md\n",
    "    setup.py\n",
    "    LICENSE\n",
    "    requirements.txt\n",
    "```\n",
    "\n",
    "Currently the only implemented module is ``` AutoDiff.py```, which contains the AutoDiff class that contains all the methods for forward mode automatic differentiation, including basic functions like exponents and trig functions, and operations like addition, multiplication, negation, etc. \n",
    "\n",
    "All functions in ```AutoDiff.py``` contain documentation and doctests. Further tests are located in the ```tests``` directory, in ```test_fwd.py``` and ```test_bp.py```. They are run whenever a push is made to the project repository through TravisCI and Coveralls. The badges for Coveralls and TravisCI are updated on our github repository README.md file, allowing easy confirmation of our package's working status.\n",
    "\n",
    "We plan on expanding the project with the back_prop module, which will contain methods that implement back propagation. Each module will also include functions that allow users to manipulate functions in automatic differentiation as objects. The back propagation will have applications in machine learning.\n",
    "\n",
    "Eventually, our package will be distributed through PyPI and will be supported with the MIT License. For now, however, users can download all necessary files from our project repository, install the dependencies outlined in ```requirements.txt```, and use the modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "This package implements an automatic differentiation class, which defines basic functions (e.g. addition) via operator overloading. The AD class will have name attributes for the function and derivative at a point. In addition, the AD class will have method attributes (defined using operator overloading) for addition, subtraction, multiplication, and division, etc. for instances of the class. The function value attribute can be computed via built-in methods, without operator overloading. The core data structures are floats and numpy arrays in order to hold the function value and derivative at a point. Elementary functions such as `sin` and `exp` will be defined outside of the AD class, relying on numpy functions such as `np.sin()`.\n",
    "\n",
    "Our package relies on numpy for vector manipulations when necessary.\n",
    "\n",
    "### Future \n",
    "Our current implementation can handle scalar functions of scalar values. We will extend our class to work with vector function of vectors in the future. We will also ensure that as many mathematical operations as possible are covered by our module (e.g. we do not currently support inverse trig functions or hyperbolic functions). \n",
    "\n",
    "As we have not attempted vector functions yet, we anticipate that this may be a challenge. An additional challenge will be to extend our current setup to accomodate a back-propagation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
