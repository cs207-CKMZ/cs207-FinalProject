{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "We are aiming to develop a software library for ***automatic differentiation*** (AD), which is a technique to numerically evaluate the derivative of a given function.\n",
    "\n",
    "Derivative is the key to many scientific and engineering problems, like numerical simulations and optimizations, so that the accuracy and speed of its evaluation are very important. Unlike symbolic or numerical differentiation, AD is much more computationally efficient with less errors and it is much faster at computing partial derivatives of functions with many inputs, which makes AD a superior method for derivative evaluations in large-scale scientific problems to other classic methods.\n",
    "\n",
    "### Background\n",
    "**Decomposition into elementary functions**\n",
    "\n",
    "One of the basic ideas behind AD is that given any complicated functions, we can decompose it into elementary ones. In other words, we can consider any function as a combination of elementary functions with elementary arithmetic operations, like addition and multiplication. The elementary functions include but are not limited to powers ($x^k$), roots ($\\sqrt{x}$), exponential functions ($e^{x}$), logarithms ($\\log{x}$), trigonometric functions ($\\sin{x}$), etc.\n",
    "\n",
    "**Computational graph**\n",
    "\n",
    "The realization of function decomposition is through computational graph. Each node represents an elementary arithmetic operation, or an elementary function, while each directed edge represents a path, or the flow of information. The computational graph can be used for both function value evaluation and derivative evaluation, where function value evalution only requires edge to deliver the value of the source node, while derivative evaluation also stores derivative on edges.\n",
    "\n",
    "**Chain rule**\n",
    "\n",
    "The fundamental to AD is the decomposition of differentials provided by the chain rule, which has the following formula：\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial t} = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial t},$$ where we have a function $f = f(x(t))$. By applying the chain rule repeatedly on the computational graph, derivatives of arbitrary order can be computed automatically and accurately with low computational complexity。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use *AutoDiff_CKMZ*\n",
    "\n",
    "As our package will be distributed through PyPI, users can install it using `pip`:\n",
    "```\n",
    "pip install AutoDiff_CKMZ\n",
    "```\n",
    "\n",
    "To use forward mode of automatic differentiation, users should import the Fwd_AD module. To use the reverse mode of automatic differentiation, Rev_AD module should be imported. They can also import all the modules in the package.\n",
    "```\n",
    "from AutoDiff_CKMZ.modules import AutoDiff\n",
    "from AutoDiff_CKMZ.modules import back_prop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Organization\n",
    "\n",
    "The directory structure will look like \n",
    "```\n",
    "AutoDiff_CKMZ/\n",
    "    AutoDiff_CKMZ/\n",
    "        README.md\n",
    "        setup.py\n",
    "        LICENSE\n",
    "        requirements.txt\n",
    "        modules/  \n",
    "            __init__.py\n",
    "            AutoDiff.py\n",
    "            back_prop.py\n",
    "            tests/\n",
    "                __init__.py\n",
    "                test_fwd_simple.py\n",
    "                test_fwd_complicated.py\n",
    "                test_bp.py\n",
    "```\n",
    "\n",
    "Currently the only implemented module is ``` AutoDiff.py```, which contains the AutoDiff class that contains all the methods for forward mode automatic differentiation, including basic functions like exponents and trig functions, and operations like addition, multiplication, negation, etc. \n",
    "\n",
    "All functions in ```AutoDiff.py``` contain documentation and doctests. Further tests are located in the ```tests``` directory, in ```test_fwd_simple.py``` and ```test_fwd_complicated.py```. They are run whenever a push is made to the project repository through TravisCI and Coveralls. The badges for Coveralls and TravisCI are updated on our github repository README.md file, allowing easy confirmation of our package's working status.\n",
    "\n",
    "We plan on expanding the project with the back_prop module, which will contain methods that implement back propagation. Each module will also include functions that allow users to manipulate functions in automatic differentiation as objects. The back propagation will have applications in machine learning.\n",
    "\n",
    "Eventually, our package will be distributed through PyPI and will be supported with the MIT License. For now, however, users can download all necessary files from our project repository, install the dependencies outlined in ```requirements.txt```, and use the modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Discuss how you plan on implementing the forward mode of automatic differentiation.\n",
    "* What are the core data structures?\n",
    "* What classes will you implement?\n",
    "* What method and name attributes will your classes have?\n",
    "* What external dependencies will you rely on?\n",
    "* How will you deal with elementary functions like `sin` and `exp`?\n",
    "\n",
    "Be sure to consider a variety of use cases.  For example, don't limit your design to scalar\n",
    "functions of scalar values.  Make sure you can handle the situations of vector functions of vectors and\n",
    "scalar functions of vectors.  Don't forget that people will want to use your library in algorithms\n",
    "like Newton's method (among others).\n",
    "\n",
    "Try to keep your report to a reasonable length.  It will form the core of your documentation, so you\n",
    "want it to be a length that someone will actually want to read.\n",
    "\n",
    "\n",
    "* __What are the core data structures?__\n",
    "\n",
    "We will use tuples for the dual number class to hold the function value and derivative at a point. Each tuple element could be a scalar or a numpy array, depending on the scalar/vector nature of the function we are differentiating.\n",
    "\n",
    "* __What classes will you implement?__\n",
    "\n",
    "We will implement two classes: (1) a dual number class and (2) an automatic differentiation class, which will inherit from the dual number class.\n",
    "\n",
    "* __What method and name attributes will your classes have?__\n",
    "\n",
    "The dual number class will have name attributes for the function and derivative at a point. As the automatic differentiation class inhertis from the dual number class, the AD class will also have name attributes for the function and derivative values at a point. In addition, the AD class will have method attributes (defined using operator overloading) for addition, subtraction, multiplication, and division, etc. of the derivative attribute of the class. The function value attribute can be computed via built-in methods, without operator overloading. \n",
    "\n",
    "* __What external dependencies will you rely on?__\n",
    "\n",
    "We will rely on numpy arrays for vector manipulations when necessary.\n",
    "\n",
    "* __How will you deal with elementary functions like `sin` and `exp`?__\n",
    "\n",
    "Elementary functions such as `sin` and `exp` will be defined manually via operator overloading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
