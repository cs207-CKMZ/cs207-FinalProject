{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "We are aiming to develop a software library for ***automatic differentiation*** (AD), which is a technique to numerically evaluate the derivative of a given function.\n",
    "\n",
    "Derivative is the key to many scientific and engineering problems, like numerical simulations and optimizations, so that the accuracy and speed of its evaluation are very important. Unlike symbolic or numerical differentiation, AD is much more computationally efficient with less errors and it is much faster at computing partial derivatives of functions with many inputs, which makes AD a superior method for derivative evaluations in large-scale scientific problems to other classic methods.\n",
    "\n",
    "### Background\n",
    "**Decomposition into elementary functions**\n",
    "\n",
    "One of the basic ideas behind AD is that given any complicated functions, we can decompose it into elementary ones. In other words, we can consider any function as a combination of elementary functions with elementary arithmetic operations, like addition and multiplication. The elementary functions include but are not limited to powers ($x^k$), roots ($\\sqrt{x}$), exponential functions ($e^{x}$), logarithms ($\\log{x}$), trigonometric functions ($\\sin{x}$), etc.\n",
    "\n",
    "**Computational graph**\n",
    "\n",
    "The realization of function decomposition is through computational graph. Each node represents an elementary arithmetic operation, or an elementary function, while each directed edge represents a path, or the flow of information. The computational graph can be used for both function value evaluation and derivative evaluation, where function value evalution only requires edge to deliver the value of the source node, while derivative evaluation also stores derivative on edges.\n",
    "\n",
    "**Chain rule**\n",
    "\n",
    "The fundamental to AD is the decomposition of differentials provided by the chain rule, which has the following formula：\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial t} = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial t},$$ where we have a function $f = f(x(t))$. By applying the chain rule repeatedly on the computational graph, derivatives of arbitrary order can be computed automatically and accurately with low computational complexity。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use *AutoDiff_CKMZ*\n",
    "\n",
    "As our package will be distributed through PyPI, users can install it using `pip`:\n",
    "```\n",
    "pip install AutoDiff_CKMZ\n",
    "```\n",
    "\n",
    "To use forward mode of automatic differentiation, users should import the Fwd_AD module. To use the reverse mode of automatic differentiation, Rev_AD module should be imported. They can also import all the modules in the package.\n",
    "```\n",
    "from AutoDiff_CKMZ.modules import Fwd_AD \n",
    "from AutoDiff_CKMZ.modules import Rev_AD\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Organization\n",
    "Discuss how you plan on organizing your software package.\n",
    "* What will the directory structure look like?  \n",
    "* What modules do you plan on including?  What is their basic functionality?\n",
    "* Where will your test suite live?  Will you use `TravisCI`? `Coveralls`?\n",
    "* How will you distribute your package (e.g. `PyPI`)?\n",
    "\n",
    "The directory structure will look like \n",
    "```\n",
    "AutoDiff_CKMZ/\n",
    "    AutoDiff_CKMZ/\n",
    "        README.md\n",
    "        setup.py\n",
    "        LICENSE\n",
    "        modules/  \n",
    "            __init__.py\n",
    "            Fwd_AD.py\n",
    "            Rev_AD.py\n",
    "            tests/\n",
    "                __init__.py\n",
    "                test_fwd.py\n",
    "                test_rev.py\n",
    "```\n",
    "\n",
    "We plan on including the Fwd_AD and Rev_AD modules, which will contain methods that implement the forward and reverse mode of automatic differentiation. Each module will also include functions that allow users to manipulate functions in automatic differentiation as objects.\n",
    "\n",
    "We will use both TravisCI and Coveralls to encourage test-driven development and continually make sure new code passes all the tests (i.e. that it does not lose its basic functionality). The badges for Coveralls and TravisCI are updated on our github repository README.md file, allowing easy confirmation of our package's working status. Each module will have a test suite, and all test suites will be stored in the ```tests``` directory in our package.\n",
    "\n",
    "Our package will be distributed through PyPI and will be supported with the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Discuss how you plan on implementing the forward mode of automatic differentiation.\n",
    "* What are the core data structures?\n",
    "* What classes will you implement?\n",
    "* What method and name attributes will your classes have?\n",
    "* What external dependencies will you rely on?\n",
    "* How will you deal with elementary functions like `sin` and `exp`?\n",
    "\n",
    "Be sure to consider a variety of use cases.  For example, don't limit your design to scalar\n",
    "functions of scalar values.  Make sure you can handle the situations of vector functions of vectors and\n",
    "scalar functions of vectors.  Don't forget that people will want to use your library in algorithms\n",
    "like Newton's method (among others).\n",
    "\n",
    "Try to keep your report to a reasonable length.  It will form the core of your documentation, so you\n",
    "want it to be a length that someone will actually want to read.\n",
    "\n",
    "\n",
    "* __What are the core data structures?__\n",
    "\n",
    "We will use tuples for the dual number class to hold the function value and derivative at a point. Each tuple element could be a scalar or a numpy array, depending on the scalar/vector nature of the function we are differentiating.\n",
    "\n",
    "* __What classes will you implement?__\n",
    "\n",
    "We will implement two classes: (1) a dual number class and (2) an automatic differentiation class, which will inherit from the dual number class.\n",
    "\n",
    "* __What method and name attributes will your classes have?__\n",
    "\n",
    "The dual number class will have name attributes for the function and derivative at a point. As the automatic differentiation class inhertis from the dual number class, the AD class will also have name attributes for the function and derivative values at a point. In addition, the AD class will have method attributes (defined using operator overloading) for addition, subtraction, multiplication, and division, etc. of the derivative attribute of the class. The function value attribute can be computed via built-in methods, without operator overloading. \n",
    "\n",
    "* __What external dependencies will you rely on?__\n",
    "\n",
    "We will rely on numpy arrays for vector manipulations when necessary.\n",
    "\n",
    "* __How will you deal with elementary functions like `sin` and `exp`?__\n",
    "\n",
    "Elementary functions such as `sin` and `exp` will be defined manually via operator overloading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
